# Tool Compare 评估指令

当用户提到 “Tool Compare” 或引用本文件时，立即加载此指令集。你是主控 Codex，负责对功能相近或互为备选的工具开展系统化评估，并给出可落地的选型建议。保持默认模型与环境配置（除非获授权修改），所有实验都要记录时间（参考 UTC-7，当日为 2025-10-18）、工具版本、成本与来源。

> 说明：本指令中的“工具”指的是**当前 Agent 会话中可见并可直接调用的内置工具**（例如 `web.run`、`tavily__tavily_search`、`exa__web_search_exa` 等）。评估时必须覆盖这些内置工具的相关候选项，除非工具因权限或配额无法调用，并在计划中记录原因。

## 角色与目标
- 独立完成计划、执行、分析与交付，无需等待用户确认。除非用户明确取消记录要求，否则整个流程必须产出完整日志与 artefact。
- 覆盖核心指标：任务完成度（命中率/准确性）、token 使用效率、响应延迟、成本、噪音/付费墙比例、主要来源可信度，以及用户指定的额外指标。
- 提供可追溯的证据（原始输出、日志、引用），明确领先者、短板、风险与下一步行动。

## 核心原则
1. **先定标准再实验**：在 `evaluation_plan.md` 中列出候选工具、样本查询、指标、验证方法与 artefact 清单。
2. **统一输入**：对所有工具保持一致的查询、参数、时间窗口；若必须调整，记录原因与影响。
3. **数据留痕**: 每次调用写入结构化日志，原始响应、聚合结果与脚本均存档，确保他人可复现。
4. **证据驱动结论**: 报告中所有判断均指向具体日志或外部来源，并给出信心等级。

## 标准流程
1. **准备**
   - 在 `evals/search/<日期>/evaluation_plan.md` 定义样本、指标、执行脚本、凭据位置及风险假设。
   - 建立数据目录：`artifacts/tool_compare/<日期>/raw/`, `processed/`, `scripts/`。
   - 确认候选工具凭据、配额、速率限制，记录于计划中。

2. **执行**
   - 编写或复用批量脚本（推荐 Python/TypeScript），确保支持重试、超时（默认 15s，可调）、并发控制。
   - 对照样本依次调用各工具，实时将日志写入 `raw/<tool>.jsonl`；同步在 `evals/search/<日期>/process.md` 记录步骤与异常。
   - 冒烟样本也纳入日志，不得跳记。

3. **分析**
   - 在 `scripts/` 内运行聚合逻辑，生成 `processed/latency.csv`, `metrics.csv`, `noise.csv` 等结果。
   - 计算命中率、平均延迟、P95 延迟、token/成本、噪音率、多语表现等，并按需要扩展额外指标。
   - 整理关键观察（文档质量、生态支持、合规要素）并在 `process.md` 中注明证据来源。

4. **交付**
   - 完成 `evals/search/<日期>/process.md`（执行详情、异常、待办）与 `summary.md`（核心对比表、洞察、推荐、风险）。
   - 在 `summary.md` 的对比表中列出至少三项定量指标，并写明证据路径与信心等级。
   - 报告中明确领先者、适用场景、缺陷与后续验证计划，将 artefact 路径反馈给用户。

## Artefact 约定
- `evals/search/<日期>/evaluation_plan.md`：计划与指标
- `evals/search/<日期>/process.md`：执行日志、异常、脚本说明
- `evals/search/<日期>/summary.md`：结论、对比表、建议
- `artifacts/tool_compare/<日期>/raw/`：逐调用 JSON 行日志、原始响应
- `artifacts/tool_compare/<日期>/processed/`：聚合数据表、可视化素材
- `artifacts/tool_compare/<日期>/scripts/`：采集与分析脚本（需可执行权限）

## 日志格式
每条调用至少包含：
```
timestamp, tool, query_or_task, params, latency_ms,
status, primary_sources, notes, cost_unit, cost_amount,
tokens_prompt, tokens_completion (若适用)
```
如有其他元数据（速率限制、响应大小等）一并记录。

## 指标说明
- **命中率 / 可操作性**：样本数中满足任务目标的比例。
- **token 效率**：平均 token 消耗或成本 / 成功答案数。
- **延迟**：平均值与 P95；必要时记录首字节延迟。
- **噪音率**：包含不相关结果、付费墙、断链的比例。
- **来源可信度**：主要引用是否来自权威/官方渠道。
- **多语 / 过滤能力**：若样本涉及非中文或特定过滤，记录达成情况。
- **用户指定指标**：在计划与日志中显式标注（例如法规覆盖、地理可用性）。

## 异常处理
- 任意失败、超时、配额不足或脚本异常，都要在日志与 `process.md` 登记原因、重试情况及结果。
- 若缺失数据 ≥30%，在 `summary.md` 中视为未完成并给出补救计划。

## 交付检查清单
- [ ] `evaluation_plan.md` 填写完整并与执行一致
- [ ] `raw/` 日志覆盖全部样本与工具
- [ ] `processed/` 含核心指标数据
- [ ] `process.md` 记录关键步骤、异常、学习点
- [ ] `summary.md` 出具对比表、洞察、推荐、风险
- [ ] 所有结论均可追溯到日志或外部引用
